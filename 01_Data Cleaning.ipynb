{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning The Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data_df = pd.read_csv(\"lyrics.csv\",index_col=0)\n",
    "data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def clean_text_1(text):\n",
    "    # Lowercase\n",
    "    text = text.lower()\n",
    "    # Remove special text in brackets ([chorus],[guitar],etc)\n",
    "    text = re.sub('\\[.*?\\]', '', text)\n",
    "    # Remove punctuation\n",
    "    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n",
    "    # Remove words containing numbers\n",
    "    text = re.sub('\\w*\\d\\w*', '', text)    \n",
    "    # Remove quotes\n",
    "    text = re.sub('[‘’“”…]', '', text)\n",
    "    # Remove new line \\n \n",
    "    text = re.sub('\\n', ' ', text)\n",
    "    # Remove stop_word\n",
    "    stop_words = stopwords.words('english')\n",
    "    words = word_tokenize(text)\n",
    "    new_text = \"\"\n",
    "    for w in words:\n",
    "        if w not in stop_words and len(w) > 1:\n",
    "            new_text = new_text + \" \" + w\n",
    "    return new_text\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's take a look at the updated text\n",
    "data_clean = pd.DataFrame(data_df.lyrics.apply(clean_text_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df.loc['David_Bowie']['lyrics'][:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_clean.loc['David_Bowie']['lyrics'][:500]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming / Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#A list of words to be stemmed\n",
    "verb_list = [\"was\",\"were\",\"am\",\"general\",\"generalize\",\"generalizing\",\"insurance\",\"insured\"]\n",
    "noun_list = [\"dogs\",\"feet\",\"insurance\",\"knowledge\"]\n",
    "adjec_list= [\"harder\",\"better\",\"faster\",\"stronger\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "porter = PorterStemmer()\n",
    "lancaster = LancasterStemmer()\n",
    "\n",
    "print(\"%-20s %-20s %-20s\"% (\"Word\",\"Porter Stemmer\",\"lancaster Stemmer\"))\n",
    "for word in verb_list:\n",
    "    print(\"%-20s %-20s %-20s\"%(word, porter.stem(word),lancaster.stem(word)))\n",
    "for word in noun_list:\n",
    "    print(\"%-20s %-20s %-20s\"%(word, porter.stem(word),lancaster.stem(word)))\n",
    "for word in adjec_list:\n",
    "    print(\"%-20s %-20s %-20s\"%(word, porter.stem(word),lancaster.stem(word)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "\n",
    "# Init the Wordnet Lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "print(lemmatizer.lemmatize(\"generalized\")) # try pos='v':\n",
    "#POS: part of speech ADJ=a, ADJ_SAT=s, ADV=r, NOUN=n, VERB=v\n",
    "\n",
    "print(\"%-20s  %-20s\"% (\"Word\",\"WordNet Lemmatizer\"))\n",
    "for word in verb_list:\n",
    "    print(\"%-20s %-20s\"%(word,lemmatizer.lemmatize(word, pos='v')))\n",
    "for word in noun_list:\n",
    "    print(\"%-20s %-20s\"%(word,lemmatizer.lemmatize(word, pos='n')))\n",
    "for word in adjec_list:\n",
    "    print(\"%-20s %-20s\"%(word,lemmatizer.lemmatize(word, pos='a')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# POS tagging\n",
    "\n",
    "from nltk import word_tokenize, pos_tag\n",
    "\n",
    "txt = \"Remember when you were young, you shone like the sun Shine on you crazy diamond\"\n",
    "pos_tag(word_tokenize(txt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "POS tag list:\n",
    "\n",
    "- CC\tcoordinating conjunction\n",
    "- CD\tcardinal digit\n",
    "- DT\tdeterminer\n",
    "- EX\texistential there (like: \"there is\" ... think of it like \"there exists\")\n",
    "- FW\tforeign word\n",
    "- IN\tpreposition/subordinating conjunction\n",
    "- JJ\tadjective\t'big'\n",
    "- JJR\tadjective, comparative\t'bigger'\n",
    "- JJS\tadjective, superlative\t'biggest'\n",
    "- LS\tlist marker\t1)\n",
    "- MD\tmodal\tcould, will\n",
    "- NN\tnoun, singular 'desk'\n",
    "- NNS\tnoun plural\t'desks'\n",
    "- NNP\tproper noun, singular\t'Harrison'\n",
    "- NNPS\tproper noun, plural\t'Americans'\n",
    "- PDT\tpredeterminer\t'all the kids'\n",
    "- POS\tpossessive ending\tparent\\'s\n",
    "- PRP\tpersonal pronoun\tI, he, she\n",
    "- PRP\\$ \tpossessive pronoun\tmy, his, hers\n",
    "- RB\tadverb\tvery, silently,\n",
    "- RBR\tadverb, comparative\tbetter\n",
    "- RBS\tadverb, superlative\tbest\n",
    "- RP\tparticle\tgive up\n",
    "- TO\tto\tgo 'to' the store.\n",
    "- UH\tinterjection\terrrrrrrrm\n",
    "- VB\tverb, base form\ttake\n",
    "- VBD\tverb, past tense\ttook\n",
    "- VBG\tverb, gerund/present participle\ttaking\n",
    "- VBN\tverb, past participle\ttaken\n",
    "- VBP\tverb, sing. present, non-3d\ttake\n",
    "- VBZ\tverb, 3rd person sing. present\ttakes\n",
    "- WDT\twh-determiner\twhich\n",
    "- WP\twh-pronoun\twho, what\n",
    "- WP\\$\tpossessive wh-pronoun\twhose\n",
    "- WRB\twh-abverb\twhere, when"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "def lemmatize_tag(text):\n",
    "    lemma=[]\n",
    "    for i,j in pos_tag(word_tokenize(text)) :\n",
    "        p=j[0].lower()\n",
    "        if p in ['j','n','v']:\n",
    "            if p == 'j':\n",
    "                p = 'a'\n",
    "            lemma.append(wnl.lemmatize(i,p))\n",
    "        else :\n",
    "            lemma.append(wnl.lemmatize(i))    \n",
    "    return ' '.join(lemma)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df.loc['David_Bowie']['lyrics'][:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_clean.loc['David_Bowie']['lyrics'][:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_clean = pd.DataFrame(data_clean.lyrics.apply(lemmatize_tag))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_clean.loc['David_Bowie']['lyrics'][:500]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save clean data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_clean.to_csv('lyrics_clean.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Question\n",
    "1. Which further clean can be aplied to the text?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
